{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS  =  Ridge Regression is a regularized linear regression technique that adds a penalty term to the ordinary least squares regression. This penalty term helps prevent overfitting by shrinking the coefficients towards zero. Unlike ordinary least squares regression, Ridge Regression adds a penalty term that depends on the square of the magnitude of coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS = Assumptions of Ridge Regression include: linearity between predictors and target variable, no perfect multicollinearity, homoscedasticity (constant variance of errors), and normally distributed errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS  =  The value of the tuning parameter (lambda) in Ridge Regression is typically selected using techniques like cross-validation. The value of lambda that minimizes the validation error is chosen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS  =  Yes, Ridge Regression can be used for feature selection by penalizing the coefficients of less important features, effectively shrinking them towards zero. Features with coefficients close to zero can be considered less important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS  = Ridge Regression performs well in the presence of multicollinearity because it shrinks the coefficients of correlated predictors towards zero, reducing their impact on the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS = Yes, Ridge Regression can handle both categorical and continuous independent variables. Categorical variables are typically converted into dummy variables before being used in the regression model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS =  Coefficients of Ridge Regression represent the relationship between independent variables and the target variable, taking into account the penalty term. Larger coefficients indicate stronger relationships, but they are penalized to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS  =  Yes, Ridge Regression can be used for time-series data analysis by treating time as an independent variable. However, other techniques like autoregressive integrated moving average (ARIMA) or seasonal autoregressive integrated moving average (SARIMA) models are more commonly used for time-series analysis."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
